{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amygeorge/flask_app/blob/master/Assignment_5_Advanced_Text_Classification_(CPSC_436N).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# CPSC 436N - Assignment 5\n",
        "# Bebo Elhosary 20375168\n",
        "# Amy George 18392134\n",
        "---"
      ],
      "metadata": {
        "id": "JrpSrPCmSYTh"
      },
      "id": "JrpSrPCmSYTh"
    },
    {
      "cell_type": "markdown",
      "id": "c5842d2f",
      "metadata": {
        "id": "c5842d2f"
      },
      "source": [
        "# Assignment 5: Advanced Text Classification - Transformers and Syntactic Features (CPSC 436N)\n",
        "\n",
        "This assigment can be seen as a continuation of assigment 3. You will still work on the task of text classification by while in assignment 3 you studied \n",
        " a Bag Of Word (BOW), and a word embedding-based classifiers, in this assigment you will develp a text classifier based on a  pretrained language model (PrLM) and one based on syntactic (POS and dependency) features.\n",
        "\n",
        "You will test these implemented classifiers on subsets of the 20newsgroup corpus, and analyze the errors and successes of one model compared to the other.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db16ab35",
      "metadata": {
        "id": "db16ab35"
      },
      "source": [
        "## 1. Install Spacy and language models will be used in later steps:\n",
        "\n",
        "#### Install Spacy v3.0: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bc1p1lcRXPAv",
      "metadata": {
        "id": "Bc1p1lcRXPAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5c8f72a-5beb-450d-cfd1-dd3c26304bc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy==3.0\n",
            "  Downloading spacy-3.0.0-cp37-cp37m-manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (2.23.0)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (1.21.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (4.11.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (0.9.0)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "  Downloading pydantic-1.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 17.1 MB/s \n",
            "\u001b[?25hCollecting thinc<8.1.0,>=8.0.0\n",
            "  Downloading thinc-8.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (653 kB)\n",
            "\u001b[K     |████████████████████████████████| 653 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (2.11.3)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (1.0.6)\n",
            "Collecting srsly<3.0.0,>=2.4.0\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 36.8 MB/s \n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.0\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (3.0.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (3.10.0.2)\n",
            "Collecting pathy\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (4.63.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (21.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (0.4.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy==3.0) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.0) (3.0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0) (3.0.4)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy==3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.0) (2.0.1)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy->spacy==3.0) (5.2.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-legacy, pathy, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 pathy-0.6.1 pydantic-1.7.4 spacy-3.0.0 spacy-legacy-3.0.9 srsly-2.4.2 thinc-8.0.15 typer-0.3.2\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "!{sys.executable} -m pip install spacy==3.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Igaf9WsjXeLu",
      "metadata": {
        "id": "Igaf9WsjXeLu"
      },
      "source": [
        "#### Download and install two trained English pipelines ([en_core_web_lg](https://spacy.io/models/en) and [en_core_web_trf](https://spacy.io/models/en)) provided by Spacy: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U1C3Ur4MXsI9",
      "metadata": {
        "id": "U1C3Ur4MXsI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62706421-c1c2-42d0-e36e-ad72a7bc8061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.0.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0-py3-none-any.whl (778.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 778.8 MB 20 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.10.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.0.6)\n",
            "Requirement already satisfied: pathy in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.6.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.21.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.9.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.4.2)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.7.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (8.0.15)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (4.11.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (21.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (4.63.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (5.2.1)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eFgU9lKIX30l",
      "metadata": {
        "id": "eFgU9lKIX30l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb4a3a4-cc0d-4338-a6f9-0e7b458e4119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-trf==3.0.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.0.0/en_core_web_trf-3.0.0-py3-none-any.whl (459.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 459.7 MB 17 kB/s \n",
            "\u001b[?25hCollecting spacy-transformers<1.1.0,>=1.0.0rc4\n",
            "  Downloading spacy_transformers-1.0.6-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 837 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-trf==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (21.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (4.11.2)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.10.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.0.6)\n",
            "Requirement already satisfied: pathy in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (4.63.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (8.0.15)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.21.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.7.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.9.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2021.10.8)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2\n",
            "  Downloading spacy_alignments-0.8.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 9.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (1.10.0+cu111)\n",
            "Collecting spacy-transformers<1.1.0,>=1.0.0rc4\n",
            "  Downloading spacy_transformers-1.0.5-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
            "\u001b[?25h  Downloading spacy_transformers-1.0.4-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting transformers<4.10.0,>=3.4.0\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 37.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 28.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (3.6.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 40.9 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 42.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (5.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers, spacy-alignments, spacy-transformers, en-core-web-trf\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed en-core-web-trf-3.0.0 huggingface-hub-0.0.12 pyyaml-6.0 sacremoses-0.0.49 spacy-alignments-0.8.4 spacy-transformers-1.0.4 tokenizers-0.10.3 transformers-4.9.2\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_trf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libs needed for utilizing GPU.\n",
        "from thinc.api import set_gpu_allocator, require_gpu, prefer_gpu\n",
        "set_gpu_allocator(\"pytorch\")\n",
        "\n",
        "require_gpu(0)\n",
        "\n",
        "import torch\n",
        "#import torch# If there's a GPU available...\n",
        "if torch.cuda.is_available():  # Tell PyTorch to use the GPU. \n",
        " device = torch.device(\"cuda\") \n",
        " print('There are %d GPU(s) available.' % torch.cuda.device_count()) \n",
        " print('We will use the GPU:', torch.cuda.get_device_name(0)) # If not...\n",
        "else:\n",
        " print('No GPU available, using the CPU instead.')\n",
        " device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "j4p8miII1QbG",
        "outputId": "d1af7e84-2aae-4f17-b19c-e771bb1bf535"
      },
      "id": "j4p8miII1QbG",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b548499",
      "metadata": {
        "id": "7b548499"
      },
      "source": [
        "## 2. Load dataset (20newsgroups) (same as assignemt 3)\n",
        "\n",
        "For this assignment, we train and test classification models on the 20newsgroups corpus, which can be easily fetched by sklearn. This dataset comprises around 18000 newsgroups posts on 20 topics and has been splited into two subsets by sklearn for model training and testing. \n",
        "\n",
        "To ensure this assignment is manageable and won't take too long for training and inference, we will use a subset of 20newsgroups only covering samples belonging to either one of the two classes (like '__talk.politics.misc__', '__talk.religion.misc__' used below). With this setting, we simplify the multiclass classification to binary classification. \n",
        "\n",
        "If you need a refresher from assignment3, please refer the following two links for more details about 20newsgroups corpus and how to load and process it with sklearn:\n",
        "\n",
        "* http://qwone.com/~jason/20Newsgroups/\n",
        "* https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4dc4d1f2",
      "metadata": {
        "id": "4dc4d1f2"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=['talk.politics.misc','talk.religion.misc'])\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=['talk.politics.misc','talk.religion.misc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eae9dd1",
      "metadata": {
        "id": "0eae9dd1"
      },
      "source": [
        "## 3. Tokenizer is created with Spacy as in assignment-3:\n",
        "\n",
        "Tokenization is a critical preprocessing step when we work with text data. What [tokenization](https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/) does is separating input text into tokens, which can be either words, characters, or subwords, and further processing these tokens to reduce the noise caused by informal expressions and typos contained in text.\n",
        "\n",
        "There are different ways to customize your tokenization strategy. Some typical steps include __lowercasing__, __stop words and punctuations removing__, __lemmatization__ and __filtering tokens with part-of-speech tagging__.\n",
        "\n",
        "In the code cell below, the tokenization function with Spacy (named as __spacy_tokenizer__) covers:\n",
        "\n",
        "* Lowercasing\n",
        "\n",
        "* Removing stop words\n",
        "\n",
        "* Removing punctuations\n",
        "\n",
        "* Lemmatization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b47c6ae3",
      "metadata": {
        "id": "b47c6ae3"
      },
      "outputs": [],
      "source": [
        "#import libs needed for tokenization.\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "import string\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')  # Load trained English pipeline \"en_core_web_lg\".\n",
        "punctuations = string.punctuation  # Get the list of punctuation.\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS  # Get the list of stop words identified in the loaded language model.\n",
        "\n",
        "# Creating your own tokenizer function with functions built in Spacy.\n",
        "def spacy_tokenizer(doc):\n",
        "\n",
        "    tokens = nlp(doc)  # Splits the doc into tokens and applies the loaded pipeline \n",
        "    #(create all linguistic annotations for the doc, including POS etc).\n",
        "\n",
        "    # Lemmatizing each token and converting each token into lowercase\n",
        "    tokens = [ word.lemma_.lower().strip() for word in tokens ]\n",
        "\n",
        "    # Removing stop words\n",
        "    tokens = [ word for word in tokens if word not in stop_words and word not in punctuations ]\n",
        "    \n",
        "    return tokens  # return preprocessed list of tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "920442a6",
      "metadata": {
        "id": "920442a6"
      },
      "source": [
        "## 4. Build the pipeline for the classification model with pretrained language model\n",
        "\n",
        "Different from the word embedding-based classifier we implemented in Assignment3, here we replace the document vectorizer with the pretrained neural language model \"en_core_web_trf\" (RoBERTa) provided by Spacy. You can learn more about this language model from [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf).\n",
        "\n",
        "While in Assignment3 we used the SpacyWordEmb_CPNT in the pipeline, now we need to  implement the class of document vectorization component with pretrained language model (named as __SpacyLangModel_CMPT__) and place it into the pipeline.\n",
        "\n",
        "**Q1:** Please where requested add comments to the transform function in SpacyLangModel_CMPT in the code cell below. The comments should explain the best you can what the code is doing and why.\n",
        "\n",
        "You may find the link below is useful when understanding how to use pretrained language models in Spacy:\n",
        "\n",
        "https://spacy.io/usage/embeddings-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "feb4fcb1",
      "metadata": {
        "id": "feb4fcb1"
      },
      "outputs": [],
      "source": [
        "#Now let's design the pipeline for the text classification model with sklearn!\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# import libs and classes needed for component construction.\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "\n",
        "# define the class of the preprocessing component (same as in assignment 3)\n",
        "class Preprocessing_CPNT(BaseEstimator, TransformerMixin):\n",
        "    def transform(self, X, **transform_params): # the function actually performs the preprocessing, X contains all samples in the input corpus.\n",
        "            return [spacy_tokenizer(text) for text in X] # apply the spacy tokenizer we implemented above to preprocess all input data.\n",
        "\n",
        "    # in this case, we don't need to calculate values for later scaling, so fit() is doing nothing but just return self.\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "# set classifier to Logistic Regression        \n",
        "classifier = LogisticRegression(max_iter = 1000)\n",
        "\n",
        "#import libs needed for utilizing GPU.\n",
        "from thinc.api import set_gpu_allocator, require_gpu, prefer_gpu\n",
        "\n",
        "# define the class of the document vectorization component with pretrained language model\n",
        "class SpacyLangModel_CMPT(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, nlp): # define the language model and dimension of embeddings we want to use.\n",
        "        self.nlp = nlp\n",
        "        self.dim = 768\n",
        "        \n",
        "    def transform(self, X): # the function actually vectorizes samples with pretrained neural language model, X contains all samples in the input corpus.\n",
        "        X_str = []\n",
        "        for text in X:\n",
        "            X_str.append(' '.join(text)) #ADD COMMENT HERE: takes an array of tokens, and converts them to a space seperated sentence of tokens\n",
        "        return [cp.asnumpy(self.nlp(text)._.trf_data.tensors[0][0][0]) for text in X_str] #ADD COMMENT HERE: represents every sentence as a\n",
        "                # document's vector or as a vector of BPE token-pieces\n",
        "      \n",
        "    \n",
        "    # in this case, fit() is doing nothing.\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_trf\")  # Load the pretrained english pipeline (language model) \"en_core_web_trf\"\n",
        "\n",
        "# Create pipeline for the classfier with pretrained language model\n",
        "pipe = Pipeline([(\"preprocessing\", Preprocessing_CPNT()),\n",
        "                ('encoding', SpacyLangModel_CMPT(nlp)),\n",
        "                (\"classifier\", classifier)])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QR67NWkOgNX0",
      "metadata": {
        "id": "QR67NWkOgNX0"
      },
      "source": [
        "Now let's train this classification model based on pretrained language model.\n",
        "\n",
        "Please read:\n",
        "\n",
        "https://spacy.io/usage/embeddings-transformers\n",
        "\n",
        "for how to use GPU for model training and inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d924a5ef",
      "metadata": {
        "id": "d924a5ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "70a7d91a-42ce-462a-babc-59f931877524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (658 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('preprocessing', Preprocessing_CPNT()),\n",
              "                ('encoding',\n",
              "                 SpacyLangModel_CMPT(nlp=<spacy.lang.en.English object at 0x7f68ca572b90>)),\n",
              "                ('classifier', LogisticRegression(max_iter=1000))])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "\n",
        "\n",
        "# # Use GPU if available\n",
        "# if prefer_gpu():\n",
        "#   set_gpu_allocator(\"pytorch\")\n",
        "#   require_gpu(0)\n",
        "  \n",
        "# else:\n",
        "#   print(\"GPU unavailable, we do not recommend running this on a CPU.\")\n",
        "\n",
        "\n",
        "# Model Training (This may take > 5 min depending on the GPU)\n",
        "pipe.fit(newsgroups_train.data, newsgroups_train.target)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tN3FzkKPgXbs",
      "metadata": {
        "id": "tN3FzkKPgXbs"
      },
      "source": [
        "And do prediction on the testing set and evaluate the performance of this classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b6ea973f",
      "metadata": {
        "id": "b6ea973f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0fbeba13-f9b0-460c-bbda-978e4ed241ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.8128342245989305\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "# Predicting with a test dataset\n",
        "predicted_trf = pipe.predict(newsgroups_test.data)\n",
        "\n",
        "# Model Accuracy\n",
        "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(newsgroups_test.target, predicted_trf))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W1HmNHSSNoWJ",
      "metadata": {
        "id": "W1HmNHSSNoWJ"
      },
      "source": [
        "## 5. Build the pipeline for the syntax/semantic-based classifier\n",
        "\n",
        "Here we explore how to vectorize documents with two syntatic features: part-of-speech tags and syntactic dependencies. \n",
        "\n",
        "**Q2:** Why do we create the pipeline with only document vectorization and classifier component here? In other words why don't we want the spacy tokenizer implemented above to be run before we do POS tagging and dependency parsing?\n",
        "\n",
        "**Answer: We don’t run the tokenizer because we want to keep the grammar and structure of the text. Otherwise, important information for POS tagging and dependency parsing will be lost.** \n",
        "\n",
        "Now you need to implement the syntax-based document vectorization component (named as __Syntax_CMPT__) to extract pos and dependency features and use them to represent the document. \n",
        "\n",
        "**Q3:** Please where requested add comments to the transform function in __Syntax_CMPT__ in the code cell below. The comments should explain the best you can what the code is doing and why.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ad98ae8d",
      "metadata": {
        "id": "ad98ae8d"
      },
      "outputs": [],
      "source": [
        "class Syntax_CMPT(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, nlp): # define the language pipeline we want to use and vectorizers for POS and dependency respectively.\n",
        "        self.nlp = nlp\n",
        "        self.pos_vectorizer = CountVectorizer()\n",
        "        self.dep_vectorizer = CountVectorizer()\n",
        "        \n",
        "    def transform(self, X): # transform the testing data into pos+dep vectors with the pos and dependency vectorizer learned \n",
        "    #from training data.\n",
        "        pos_doc_list = []; dep_doc_list = [];\n",
        "        for text in X:\n",
        "            doc = self.nlp(text) # apply the loaded language pipeline to testing samples.\n",
        "            doc_pos = []; doc_dep = [];\n",
        "            for sent in doc.sents: #ADD COMMENT HERE: go through all the sentences in the document\n",
        "              doc_pos = doc_pos+[str(token.tag_) for token in self.nlp(str(sent))] #ADD COMMENT HERE: go through all the tokens in the sentence\n",
        "              # and append the token's tag (pos) to current doc's list of pos\n",
        "              doc_dep = doc_dep+[str(token.dep_) for token in self.nlp(str(sent))] #ADD COMMENT HERE go through all the tokens in the sentence\n",
        "              # and append the token's dependency raltion tag to current doc's list of dep tags\n",
        "            pos_doc_list.append(' '.join(doc_pos)) # turn pos tag list into a string.\n",
        "            dep_doc_list.append(' '.join(doc_dep)) # turn dependecy relation list into a string.\n",
        "\n",
        "        doc_num = len(pos_doc_list)  \n",
        "\n",
        "        X_pos = self.pos_vectorizer.transform(pos_doc_list).toarray() # transform samples into pos vectors.\n",
        "        X_dep = self.dep_vectorizer.transform(dep_doc_list).toarray() # transform samples into dep vectors.\n",
        "\n",
        "        return np.array([list(X_pos[i]) + list(X_dep[i]) for i in range(doc_num)]) # concatenate pos and dependency vectors.\n",
        "    \n",
        "\n",
        "    def fit(self, X, y=None): # different from classifiers above, this time we need to first use fit() to learn values for pos and dependency \n",
        "    #vectorizer from the training data. \n",
        "        pos_doc_list = []; dep_doc_list = [];\n",
        "        for text in X:\n",
        "            doc = self.nlp(text) # apply the loaded language pipeline to training samples.\n",
        "            doc_pos = []; doc_dep = [];\n",
        "            for sent in doc.sents: \n",
        "              doc_pos = doc_pos+[str(token.tag_) for token in self.nlp(str(sent))]\n",
        "              doc_dep = doc_dep+[str(token.dep_) for token in self.nlp(str(sent))] \n",
        "            pos_doc_list.append(' '.join(doc_pos)) # turn pos tag list into a string.\n",
        "            dep_doc_list.append(' '.join(doc_dep)) # turn dependecy relation list into a string.\n",
        "\n",
        "        doc_num = len(pos_doc_list)  \n",
        "\n",
        "        X_pos = self.pos_vectorizer.fit_transform(pos_doc_list).toarray() # fit the pos vectorizer on training data.\n",
        "        X_dep = self.dep_vectorizer.fit_transform(dep_doc_list).toarray() # fit the dependency vectorizer on training data.\n",
        "        \n",
        "        return self\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")  # Load the english pipeline \"en_core_web_lg\"\n",
        "\n",
        "# Create pipeline for the syntax-based classifier\n",
        "pipe = Pipeline([('encoding', Syntax_CMPT(nlp)),\n",
        "                (\"classifier\", classifier)])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "esirELD7hbBn",
      "metadata": {
        "id": "esirELD7hbBn"
      },
      "source": [
        "Now let's train this classification model based on Part-Of-Speech and dependency features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6obMVRXsg_sF",
      "metadata": {
        "id": "6obMVRXsg_sF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7e173193-fbb1-4a0b-8bc8-58434a0f838a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('encoding',\n",
              "                 Syntax_CMPT(nlp=<spacy.lang.en.English object at 0x7f68c84e0850>)),\n",
              "                ('classifier', LogisticRegression(max_iter=1000))])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Model Training (This may take > 50 min depending on the GPU)\n",
        "pipe.fit(newsgroups_train.data, newsgroups_train.target)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ezKtwv7Ihp-s",
      "metadata": {
        "id": "ezKtwv7Ihp-s"
      },
      "source": [
        "And do prediction on the testing set and evaluate the performance of this classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "FNQodX31m1FC",
      "metadata": {
        "id": "FNQodX31m1FC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "eeeb0821-b466-480d-d676-3555b0f74cf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.696969696969697\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# Predicting with a test dataset (This may take > 15 min depending on the GPU)\n",
        "predicted_syntax = pipe.predict(newsgroups_test.data)\n",
        "\n",
        "# Model Accuracy\n",
        "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(newsgroups_test.target, predicted_syntax))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0uJEOwI8PluF",
      "metadata": {
        "id": "0uJEOwI8PluF"
      },
      "source": [
        "## 6. Evaluate classifiers with more sophisticated metrics\n",
        "\n",
        "In the steps above, we have evaluated the performances of the two classifiers only with the accuracy score.\n",
        "\n",
        "**Q4**: Now you will have to edit the code cells for model evaluation above to add two more evaluation metrics: **Confusion matrix** and **F1 Score** (see textbook section 4.7)\n",
        "\n",
        "Please report and discuss any  insights and observations from your analysis of the results you obtained based on these metrics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import cross_val_score, cross_validate \n",
        "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_precision_recall_curve, precision_score, recall_score, f1_score, classification_report\n",
        "# Accuracy score\n",
        "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(newsgroups_test.target, predicted_syntax))\n",
        "\n",
        "# Precision = TP/(TP + FP)\n",
        "print(\"Logistic Regression Precision:\",metrics.precision_score(newsgroups_test.target, predicted_syntax))\n",
        "\n",
        "# Recall = TP/(TP + FN)\n",
        "print(\"Logistic Regression Recall:\",metrics.recall_score(newsgroups_test.target, predicted_syntax))\n",
        "\n",
        "\n",
        "# F1 = (2*precision*recall)/(precision+recall)\n",
        "print(\"Logistic Regression F1:\",metrics.f1_score(newsgroups_test.target, predicted_syntax))\n",
        "\n",
        "# confusion matrix on the data\n",
        "print(\"Logistic Regression CM:\",metrics.confusion_matrix(newsgroups_test.target, predicted_syntax))"
      ],
      "metadata": {
        "id": "Wowh52NFGST3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "90d67a2b-54c1-41de-9db6-24c49f051ac9"
      },
      "id": "Wowh52NFGST3",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.696969696969697\n",
            "Logistic Regression Precision: 0.6832579185520362\n",
            "Logistic Regression Recall: 0.601593625498008\n",
            "Logistic Regression F1: 0.6398305084745762\n",
            "Logistic Regression CM: [[240  70]\n",
            " [100 151]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment_5_Advanced_Text_Classification_(CPSC_436N).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "for_lab",
      "language": "python",
      "name": "for_lab"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}